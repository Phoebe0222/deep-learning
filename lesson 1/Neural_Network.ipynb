{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phoebe0222/deep-learning/blob/master/lesson%201/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lSHSocbuj5NW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building Neural Network from scratch"
      ]
    },
    {
      "metadata": {
        "id": "XFLAqcsxkIry",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Network has been a facsinating deep learning topic to me. As a starting point, I've decided to follow [James Loy's guide](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) to build a very simple Neural Network from scratch. This notebook is to keep track of my learning process. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F9wjXizv2o0J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The math behind it\n",
        "Following Jame's guide and also the help of [the Element of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf), I've sorted my notes. A Neural Network is a mimic of human brain with each unit being a neuron and each connection eing a synapse, but it can also be regarded as a mathematical function that maps a **input** (x) to a desired **output** (y hat). \n",
        "\n",
        "How the function maps is as the following: \n",
        "\n",
        "*   Suppose input x has p components, output y has k classes\n",
        "*   First create **derived features** by creating linear combinations of the onputs, \n",
        "\\begin{equation}  \n",
        "z_m = \\sigma(w_mx+b_m), for\\space m =1,2,...M\n",
        "\\end{equation}\n",
        "where w is a p-vector unknown parameter called  **weights**, b is known as **bias** and Ïƒ is knowns as **activation function**\n",
        "*   Then for each class, the output is calculated as the linear combination of the derived features\n",
        "\\begin{equation}  \n",
        "T_k = \\beta_k^TZ+\\beta_{0k}, for\\space k =1,2,...K\n",
        "\\end{equation}\n",
        "*   Finally the output function allows for the fianl transformation of these output vectors,\n",
        "\\begin{equation}  \n",
        "\\hat{y}=f_k(T), for \\space k =1,2,...K\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The function would then have an arbbitrary amount of hidden layers, with a set of weights and bias for each layer. The following graph shows the architecture of a multi-layer Neural Network. ![alt text](https://docs.google.com/drawings/d/e/2PACX-1vRuEUt_8_bWMT0KUyRoZwflsA405mUXxqR4g423L-d0YI84W0DCJYT9xLC-gkkjw2Gc2VNnlawT5U0q/pub?w=960&h=720)\n",
        "\n",
        "\n",
        "Choosing the right values for the weights and biases is important to the accuracy of the prediction. Naturally we want to find the values that minimise the loss function (e.g. square loss, cross-entropy). The process of finding these values from input data is also known as **training** the Neural Network. \n",
        "\n",
        "\n",
        "We can do this with many iterations, and in each iteration we update the weights and biases to produce better model. Therefore, each iteration can be broken down into 2 steps: step 1 is to calculate the perdicted output (known as **feedforward**) and step 2 is to update the weights and biases to minimise the loss function by gradient descent (known as **backpropagation**). \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Following Jame's expample, I'll use Sigmoid activation function (1/(1+exp(-v))) to build a 2-layer Neural Network.\n"
      ]
    },
    {
      "metadata": {
        "id": "gGbIftaPs4L2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1: creating a Neural Network class\n"
      ]
    },
    {
      "metadata": {
        "id": "7rNWmhTdnQOp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y):\n",
        "        self.input      = x\n",
        "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
        "        self.weights2   = np.random.rand(4,1)                 \n",
        "        self.y          = y\n",
        "        self.output     = np.zeros(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ySk-4GWgto86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: training the Neural Network\n",
        "\n",
        "feedforward: \n"
      ]
    },
    {
      "metadata": {
        "id": "7ccgQEqStyC-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feedforward(self):\n",
        "        self.layer1 = sigmoid(np.dot(self.input, self.weights1)) #activation function\n",
        "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsTjhMWPt2eo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "backpropagation: \n"
      ]
    },
    {
      "metadata": {
        "id": "0SnI65oFvcbE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " def backprop(self):\n",
        "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
        "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
        "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
        "\n",
        "        # update the weights with the derivative (slope) of the loss function\n",
        "        self.weights1 += d_weights1\n",
        "        self.weights2 += d_weights2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNmZniF-vcEF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3: putting all together with test data"
      ]
    },
    {
      "metadata": {
        "id": "LINUbozoyfWr",
        "colab_type": "code",
        "outputId": "9f01b99c-b06f-4625-8036-be9cb574e8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1+ np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1.0 - x)\n",
        "  \n",
        "  \n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y):\n",
        "        self.input      = x\n",
        "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
        "        self.weights2   = np.random.rand(4,1)                 \n",
        "        self.y          = y\n",
        "        self.output     = np.zeros(y.shape)\n",
        "        \n",
        "    def feedforward(self):\n",
        "        self.layer1 = sigmoid(np.dot(self.input, self.weights1)) #activation function\n",
        "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
        "  \n",
        "    def backprop(self):\n",
        "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
        "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
        "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
        "\n",
        "        # update the weights with the derivative (slope) of the loss function\n",
        "        self.weights1 += d_weights1\n",
        "        self.weights2 += d_weights2\n",
        "        \n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    X = np.array([[0,0,1],\n",
        "                  [0,1,1],\n",
        "                  [1,0,1],\n",
        "                  [1,1,1]])\n",
        "    y = np.array([[0],[1],[1],[0]])\n",
        "    nn = NeuralNetwork(X,y)\n",
        "\n",
        "    for i in range(1500):\n",
        "        nn.feedforward()\n",
        "        nn.backprop()\n",
        "\n",
        "    print(nn.output)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.02266983]\n",
            " [0.97714259]\n",
            " [0.98481312]\n",
            " [0.01994611]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}